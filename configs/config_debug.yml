full_data_path: source_datasets/alpaca_data_debug.json  #  {/databricks-dolly-15k_alpaca_style.json, /alpaca_davinci_data.json, /alpaca_gpt4_data.json, }
model_name_or_path: decapoda-research/llama-7b-hf  # path to pretrained foundation llm
cache_dir: ~
model_max_length: 512
val_set_size: 0  
evol_schedule_name: KCenterSampling # {KCenterSampling, LeastConfidence, MarginSampling, EntropySampling, RandomSampling, }
result_dir_name: debug # name for the folder storing all results -> we suggest including keywords of the evol_schedule, e.g., "kc_{dataset}_{other_params}"
train_args:
  optim: adamw_torch
  num_train_epochs: 1 # DEBUG
  per_device_train_batch_size: 1 # DEBUG
  per_device_eval_batch_size: 1 # DEBUG
  gradient_accumulation_steps: 1 # DEBUG
  evaluation_strategy: "no"
  save_strategy: "steps"
  save_steps: 2000
  save_total_limit: 1
  learning_rate: 2.0e-5
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 1
  fsdp: "full_shard auto_wrap" 
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  bf16: TRUE
  tf32: TRUE
init_label_num: 2  # DEBUG, 100 in actual implementation
n_round: 2 # DEBUG, 10 in actual implementation
n_query: 2  # DEBUG, 100 in actual implementation
